## Frequently Asked Questions
- The color gets inverted from the beginning of training: the authors also observed that the generator unnecessarily inverts the color of the input image early in the trianing, and then never learns to undo the inversion. In this case, two things can be tried. First, try using identity loss `--identity 1.0` or `--identity 0.1`. We observed that the identity loss makes the generator to be more conservative and make less of unnecessary changes. However, because of this, the change may not be as dramatic. Second, try smaller variance when initializing weights by changing `--init_gain`. We observed that smaller variance in weight initialization results in less color inversion. 
- Out of memory error: CycleGAN is quite memory intensive because it needs two generator networks and two discriminator networks. If you would like to generate high resolution images, you can do the following. First, train CycleGAN on cropped images of the training set. Please be careful not to change the aspect ratio or the scale of the original image, as this can lead to the training/test gap. You can usually do this by using `--resize_or_crop crop` option, or `--resize_or_crop scale_width_and_crop`. Then at test time, load only one generator to generate the results in one direction only. This greatly saves memory because you are not loading the discriminators nad the other generator in the opposite direction. You can probably input the whole image (we have done image generation of 1024x512 resolution). You can do this using `--model test --dataroot [path to the directory containing the actual images (ex. ./datasets/horse2zebra/trainA)] --model_suffix _A`. For more explanation, please see https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/test_model.py#L16. 
